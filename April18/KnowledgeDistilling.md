文章《Distilling the Knowledge in a Neural Network》发表于 NIPS15, 其核心思想是使用训练好的大规模网络的模型中提取出来的知识来指导小规模网络。该论文的解决论文，或者说背景是一个很大的DNN往往训练出来的效果会比较好，并且多个DNN进行的ensemble可以得到更好的效果，然而在实际应用中，大规模的DNN计算量偏大、延迟偏大，不适用于真实环境下的部署。

[后文参考](https://luofanghao.github.io/blog/2016/07/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20%E3%80%8ADistilling%20the%20Knowledge%20in%20a%20Neural%20Network%E3%80%8B/)
